{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d4afc9",
   "metadata": {},
   "source": [
    "# Deploying FastKafka using Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7311d5f",
   "metadata": {},
   "source": [
    "## Building docker image\n",
    "\n",
    "To build a docker image for a FastKafka project we need the following things,\n",
    "\n",
    "1. A library which is built using FastKafka.\n",
    "2. A file in which requirements are specified. It could be a requirements.txt file or a setup.py or it could even be a wheel file.\n",
    "3. A Dockerfile to build an image which will include the above two files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b9e7a",
   "metadata": {},
   "source": [
    "### Creating FastKafka code\n",
    "\n",
    "Let's create a `FastKafka` based application and write to the `application.py` file based on the [tutorial](/#tutorial)\n",
    "\n",
    "```python\n",
    "# content of the \"application.py\" file\n",
    "\n",
    "from pydantic import BaseModel, NonNegativeFloat, Field\n",
    "\n",
    "class IrisInputData(BaseModel):\n",
    "    sepal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal length in cm\"\n",
    "    )\n",
    "    sepal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal width in cm\"\n",
    "    )\n",
    "    petal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal length in cm\"\n",
    "    )\n",
    "    petal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal width in cm\"\n",
    "    )\n",
    "\n",
    "\n",
    "class IrisPrediction(BaseModel):\n",
    "    species: str = Field(..., example=\"setosa\", description=\"Predicted species\")\n",
    "\n",
    "from fastkafka import FastKafka\n",
    "\n",
    "kafka_brokers = {\n",
    "    \"localhost\": {\n",
    "        \"url\": \"localhost\",\n",
    "        \"description\": \"local development kafka broker\",\n",
    "        \"port\": 9092,\n",
    "    },\n",
    "    \"production\": {\n",
    "        \"url\": \"kafka.airt.ai\",\n",
    "        \"description\": \"production kafka broker\",\n",
    "        \"port\": 9092,\n",
    "        \"protocol\": \"kafka-secure\",\n",
    "        \"security\": {\"type\": \"plain\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "kafka_app = FastKafka(\n",
    "    title=\"Iris predictions\",\n",
    "    kafka_brokers=kafka_brokers,\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    ")\n",
    "\n",
    "iris_species = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\")\n",
    "async def on_input_data(msg: IrisInputData):\n",
    "    global model\n",
    "    species_class = model.predict([\n",
    "          [msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]\n",
    "        ])[0]\n",
    "\n",
    "    to_predictions(species_class)\n",
    "\n",
    "\n",
    "@kafka_app.produces(topic=\"predictions\")\n",
    "def to_predictions(species_class: int) -> IrisPrediction:\n",
    "    prediction = IrisPrediction(species=iris_species[species_class])\n",
    "    return prediction\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9370f",
   "metadata": {},
   "source": [
    "### Creating requirements.txt file\n",
    "\n",
    "The above code only requires fastkafka. So, we will add only fastkafka to the `requirements.txt` file but you can add additional requirements to it too.\n",
    "\n",
    "```txt\n",
    "fastkafka>=0.3.0\n",
    "```\n",
    "\n",
    "Here we are using `requirements.txt` to store the project's dependencies. But you can use other methods like `setup.py`, `pipenv` or even `wheel` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911436ab",
   "metadata": {},
   "source": [
    "### Creating Dockerfile\n",
    "\n",
    "```{ .dockerfile .annotate }\n",
    "# (1)\n",
    "FROM python:3.9-slim-bullseye\n",
    "# (2)\n",
    "WORKDIR /project\n",
    "# (3)\n",
    "COPY application.py requirements.txt /project/\n",
    "# (4)\n",
    "RUN pip install --no-cache-dir --upgrade -r /project/requirements.txt\n",
    "# (5)\n",
    "CMD [\"fastkafka\", \"run\", \"--num-workers\", \"2\", \"--kafka-broker\", \"production\", \"application:kafka_app\"]\n",
    "```\n",
    "\n",
    "1. Start from the official Python base image.\n",
    "\n",
    "2. Set the current working directory to `/project`.\n",
    "\n",
    "    This is where we'll put the `requirements.txt` file and the `application.py` file.\n",
    "\n",
    "3. Copy the `application.py` file and `requirementx.txt` file inside the `/project` directory.\n",
    "\n",
    "4. Install the package dependencies in the requirements file.\n",
    "\n",
    "    The `--no-cache-dir` option tells `pip` to not save the downloaded packages locally, as that is only if `pip` was going to be run again to install the same packages, but that's not the case when working with containers.\n",
    "\n",
    "    The `--upgrade` option tells `pip` to upgrade the packages if they are already installed.\n",
    "\n",
    "5. Set the **command** to run the `fastkafka run` command.\n",
    "\n",
    "    `CMD` takes a list of strings, each of these strings is what you would type in the command line separated by spaces.\n",
    "\n",
    "    This command will be run from the **current working directory**, the same `/project` directory you set above with `WORKDIR /project`.\n",
    "\n",
    "    We supply additional parameters `--num-workers` and `--kafka-broker` for the run command. Finally, we specify the location of our `fastkafka` app location as a command argument.\n",
    "    \n",
    "    To learn more about `fastkafka run` command please check the [cli docs](/cli/fastkafka/#fastkafka-run).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad51d39",
   "metadata": {},
   "source": [
    "### Build the Docker Image\n",
    "\n",
    "Now that all the files are in place, let's build the container image.\n",
    "\n",
    "* Go to the project directory (where your `Dockerfile` is, containing your `application.py` file).\n",
    "* Build your FastKafka image:\n",
    "```cmd\n",
    "docker build -t fastkafka_project_image .\n",
    "```\n",
    "\n",
    "This will build a docker image called `fastkafka_project_image` with `latest` tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe73a22",
   "metadata": {},
   "source": [
    "### Start the Docker Container\n",
    "\n",
    "Run a container based on the built image:\n",
    "```cmd\n",
    "docker run -d --name fastkafka_project_container fastkafka_project_image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec10a57",
   "metadata": {},
   "source": [
    "## Additional Security\n",
    "\n",
    "We recommend using [`trivy`](https://github.com/aquasecurity/trivy) to scan the built image for any vulnerabilities and to fix it based on its recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f403c",
   "metadata": {},
   "source": [
    "## Example repo\n",
    "\n",
    "A `fastkafka` based library which uses above mentioned Dockerfile to build a docker image can be found [here](https://github.com/airtai/sample_fastkafka_project/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
